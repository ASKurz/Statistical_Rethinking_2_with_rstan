# God Spiked the Integers

Load the packages.


```{r}
#| message: false
#| warning: false

# Load
library(tidyverse)
library(tidybayes)
library(rstan)
library(loo)
library(patchwork)
library(posterior)
# library(ggdag)

# Drop grid lines
theme_set(
  theme_gray() +
    theme(panel.grid = element_blank())
)
```


## Binomial regression

### Logistic regression: Prosocial chimpanzees.

Load the @silkChimpanzeesAreIndifferent2005 `chimpanzees` data.


```{r}
data(chimpanzees, package = "rethinking")
d <- chimpanzees
rm(chimpanzees)
```


Make the index variable `treatment`, a variant of which we'll be saving as a factor with labeled levels called `labels`.


```{r}
d <- d |> 
  mutate(treatment = factor(1 + prosoc_left + 2 * condition)) |> 
  # This will come in handy, later
  mutate(labels = factor(treatment,
                         levels = 1:4,
                         labels = c("r/n", "l/n", "r/p", "l/p")))
```


We can use the `count()` function to get a sense of the distribution of the conditions in the data.


```{r}
d |> 
  count(prosoc_left, condition, treatment, labels)
```

 
We start with the simple intercept-only logistic regression model, which follows the statistical formula

$$
\begin{align*}
\text{pulled_left}_i & \sim \operatorname{Binomial}(1, p_i) \\
\operatorname{logit}(p_i) & = \alpha \\
\alpha & \sim \operatorname{Normal}(0, w),
\end{align*}
$$

where $w$ is the hyperparameter for $\sigma$ the value for which we have yet to choose.

Make the `stan_data` with the `compose_data()` function.


```{r}
# Make a data set for predictions
d_pred <- d |> 
  distinct(actor, prosoc_left, condition, treatment, labels)

# print(d_pred)

# Make the primary data list
stan_data <- d |> 
  select(actor, treatment, pulled_left) |> 
  compose_data(
  n_actor = n_distinct(actor),
  n_treatment = n_distinct(treatment),
  # For predictions
  n_pred = nrow(d_pred),
  pred_actor = d_pred$actor,
  pred_treatment = d_pred$treatment)

# What?
str(stan_data)
```


The initial program will be for prior simulations.


```{r}
model_code_11.1 <- '
data {
  int<lower=1> n;                       
  array[n] int<lower=0, upper=1> pulled_left;
}
parameters {
  real a;
}
model {
  // pulled_left ~ binomial(1, inv_logit(a));
  a ~ normal(0, 10);
}
'

m11.1 <- stan(
  model_code = model_code_11.1, 
  data = stan_data,
  cores = 4, seed = 11)
```


Here's Figure 11.3.a.


```{r}
set.seed(11)

p1 <- data.frame(a = c(
  as_draws_df(m11.1) |> pull(a),
  rnorm(n = 4e3, mean = 0, sd = 1.5))) |> 
  mutate(p = plogis(a),
         omega = rep(c(10, 1.5), each = n() / 2) |> 
           as.character()) |> 
  
  ggplot(aes(x = p, color = omega, fill = omega)) +
  geom_density(adjust = 1/10, alpha = 1/2) +
  scale_color_discrete(expression(omega)) +
  scale_fill_discrete(expression(omega)) +
  labs(x = expression(italic(p)),
       subtitle = expression("a ~ Normal(0, "*omega*")"))

p1
```


Here we make the `model_code` for `m11.2`, and compile the prior predictive distribution with `stan()`.


```{r}
model_code_11.2 <- '
data {
  int<lower=1> n;                       
  int<lower=1> n_treatment;
  vector[n] treatment;
  array[n] int<lower=0, upper=1> pulled_left;
}
parameters {
  real a;
  vector[n_treatment] b;
}
model {
  // pulled_left ~ binomial(1, inv_logit(a + b[treatment]));
  a ~ normal(0, 1.5);
  b ~ normal(0, 10);
}
'

m11.2 <- stan(
  model_code = model_code_11.2, 
  data = stan_data,
  cores = 4, seed = 11)
```


Note that this model would typically have identification issues. But since we're just sampling from the prior, it isn't an issue.

Here's `m11.3`, which is also just for prior-predictive draws, but this time using $\beta_{[1:4]} \sim \mathcal N(0, 0.5)$.


```{r}
model_code_11.3 <- '
data {
  int<lower=1> n;                       
  int<lower=1> n_treatment;
  vector[n] treatment;
  array[n] int<lower=0, upper=1> pulled_left;
}
parameters {
  real a;
  vector[n_treatment] b;
}
model {
  // pulled_left ~ binomial(1, inv_logit(a + b[n_treatment]));
  a ~ normal(0, 1.5);
  b ~ normal(0, 0.5);  // This is the only new part
}
'

m11.3 <- stan(
  model_code = model_code_11.3, 
  data = stan_data,
  cores = 4, seed = 11)
```


Here we compute the probability difference estimand $p_1 - p_2$ from the prior predictive distribution of `m11.2` and `m11.3`, and save the results as `prior_m11.2_df` and `prior_m11.3_df`.


```{r}
prior_m11.2_df <- m11.2 |> 
  spread_draws(a, b[treatment]) |> 
  filter(treatment < 3) |> 
  mutate(p = plogis(a + b)) |> 
  compare_levels(p, by = treatment)

prior_m11.3_df <- m11.3 |> 
  spread_draws(a, b[treatment]) |> 
  filter(treatment < 3) |> 
  mutate(p = plogis(a + b)) |> 
  compare_levels(p, by = treatment)

# What?
head(prior_m11.2_df)
head(prior_m11.3_df)
```


Now we combine the two to make Figure 11.3.b.


```{r, fig.width = 8, fig.height = 3.25}
p2 <- bind_rows(prior_m11.2_df, prior_m11.3_df) |> 
  mutate(psi = rep(c(10, 0.5), each = n() / 2) |> 
           as.character()) |> 
  
  ggplot(aes(x = abs(p), color = psi, fill = psi)) +
  geom_density(adjust = 1/10, alpha = 1/2) +
  scale_color_discrete(expression(psi)) +
  scale_fill_discrete(expression(psi)) +
  labs(x = expression(abs(italic(p)[1]-italic(p)[2])),
       subtitle = expression("b ~ Normal(0, "*psi*")"))

# Combine
p1 | p2
```


The average *absolute* prior difference is about 0.1, very similar to McElreath's value from R code 11.9.


```{r}
prior_m11.3_df |> 
  ungroup() |> 
  summarise(mean = abs(p) |> mean())
```


The full model follows the form

$$
\begin{align*}
\text{pulled_left}_i      & \sim \operatorname{Binomial}(1, p_i) \\
\operatorname{logit}(p_i) & = \alpha_{\text{actor}[i]} + \beta_{\text{treatment}[i]} \\
\alpha_j & \sim \operatorname{Normal}(0, 1.5) \\
\beta_k  & \sim \operatorname{Normal}(0, 0.5),
\end{align*}
$$

where we now have four levels of $\alpha_j$ and seven levels of $\beta_k$. We'll call this model `m11.4`, and this time we actually sample from the posterior. In the `generated quantities` block, we're defining a vector of predicted values `p`, as well as the `log_lik` values for information criteria.


```{r}
model_code_11.4 <- '
data {
  int<lower=1> n;                       
  int<lower=1> n_actor;
  int<lower=1> n_treatment;
  int<lower=1> n_pred;
  array[n] int actor;
  array[n] int treatment;
  array[n_pred] int pred_actor;
  array[n_pred] int pred_treatment;
  array[n] int<lower=0, upper=1> pulled_left;
}
parameters {
  vector[n_actor] a;
  vector[n_treatment] b;
}
model {
  pulled_left ~ binomial(1, inv_logit(a[actor] + b[treatment]));
  a ~ normal(0, 1.5);
  b ~ normal(0, 0.5);
}
generated quantities {
  vector[n_pred] p;
  vector[n] log_lik;
  
  p = inv_logit(a[pred_actor] + b[pred_treatment]);
  for (i in 1:n) log_lik[i] = binomial_lpmf(pulled_left[i] | 1, inv_logit(a[actor[i]] + b[treatment[i]]));
}
'

m11.4 <- stan(
  model_code = model_code_11.4, 
  data = stan_data,
  cores = 4, seed = 11)
```


Check the model summary.


```{r}
print(m11.4, pars = c("a", "b"), probs = c(0.055, 0.945))
```


Here's the coefficient plot for the intercepts, transformed onto the probability scale.


```{r, fig.width = 4.5, fig.height = 1.75}
m11.4 |> 
  spread_draws(a[j]) |> 
  mutate(j = factor(j, levels = 7:1)) |> 
  
  ggplot(aes(x = plogis(a), y = j)) +
  stat_pointinterval(point_interval = mean_qi, .width = 0.89, 
                     linewidth = 1, shape = 1) +
  xlim(0:1) +
  ylab(expression(italic(j)))
```


Here's the coefficient plot for the `treatment` parameters, on the log-odds scale.


```{r, fig.width = 4.5, fig.height = 1.25}
m11.4 |> 
  spread_draws(b[k]) |> 
  left_join(d |> 
              distinct(treatment, labels) |> 
              mutate(k = as.integer(treatment)),
            by = join_by(k)) |>
  mutate(labels = fct_rev(labels)) |> 
  
  ggplot(aes(x = b, y = labels)) +
  stat_pointinterval(point_interval = mean_qi, .width = 0.89, 
                     linewidth = 1, shape = 1) +
  ylab(NULL)
```


Here are the two contrasts.


```{r, fig.width = 4.5, fig.height = 1.25}
m11.4 |> 
  spread_draws(b[k]) |> 
  pivot_wider(names_from = k, values_from = b) |> 
  mutate(db13 = `1` - `3`,
         db24 = `2` - `4`) |> 
  pivot_longer(starts_with("db")) |> 
  mutate(name = fct_rev(name)) |> 
  
  ggplot(aes(x = value, y = name)) +
  stat_pointinterval(point_interval = mean_qi, .width = 0.89, 
                     linewidth = 1, shape = 1) +
  ylab(NULL)
```


Next, we prepare for the posterior predictive check. McElreath showed how to compute empirical proportions by the levels of `actor` and `treatment` with the `by()` function. Our approach will be with a combination of `group_by()` and `summarise()`. Here's what that looks like for `actor == 1`.


```{r}
#| message: false

d |>
  group_by(actor, treatment) |>
  summarise(proportion = mean(pulled_left)) |> 
  filter(actor == 1)
```


Now we'll follow that through to make the top panel of Figure 11.4.


```{r, fig.width = 6, fig.height = 2}
#| message: false

# Wrangle
p1 <- d |>
  group_by(actor, treatment) |>
  summarise(p = mean(pulled_left)) |> 
  left_join(d |> distinct(actor, treatment, labels, condition, prosoc_left),
            by = c("actor", "treatment")) |> 
  mutate(actor = str_c("actor ", actor),
         condition = factor(condition)) |> 
  
  # Plot
  ggplot(aes(x = labels, y = p)) +
  geom_hline(yintercept = 0.5, color = "white") +
  geom_line(aes(group = prosoc_left),
            linewidth = 1/4) +
  geom_point(aes(color = condition),
             size = 2.5, show.legend = F) + 
  scale_x_discrete(NULL, breaks = NULL) +
  facet_grid("observed proportions" ~ actor)
  
# Display
p1
```


Make the bottom panel of Figure 11.4. Note how we're using our `p` parameters for this plow, which we defined in the `generated quantities` block, above.


```{r, fig.width = 6, fig.height = 2}
# Wrangle
p2 <- m11.4 |> 
  spread_draws(p[i]) |> 
  left_join(d_pred |> 
              mutate(i = 1:n()),
            by = join_by(i)) |> 
  mutate(actor = str_c("actor ", actor),
         condition = factor(condition)) |> 
  group_by(actor, prosoc_left, condition, treatment, labels) |> 
  mean_qi(p, .width = 0.89) |> 
  
  # Plot
  ggplot(aes(x = labels, y = p)) +
  geom_hline(yintercept = .5, color = "white") +
  geom_line(aes(group = prosoc_left),
            linewidth = 1/4) +
  geom_pointinterval(aes(ymin = .lower, ymax = .upper, fill = condition),
                     linewidth = 1, shape = 21) +
  scale_fill_discrete(breaks = NULL) +
  facet_grid("posterior predictions" ~ actor) +
  theme(strip.text.x = element_blank())

# Display
p2
```


Combine the two panels tomake the full Figure 11.4.


```{r, fig.width = 6, fig.height = 4}
(p1 / p2) &
  scale_y_continuous("proportion left lever", 
                     breaks = 0:2 / 2, limits = 0:1)
```


Let's make two more index variables.


```{r}
d <- d |> 
  mutate(side = prosoc_left + 1,  # right 1, left 2
         cond = condition + 1)    # no partner 1, partner 2

# What?
d |> 
  distinct(prosoc_left, condition, side, cond)
```


Update the `stan_data`.


```{r}
stan_data <- d |> 
  select(actor, side, cond, pulled_left) |> 
  compose_data(
  n_actor = n_distinct(actor),
  n_side = n_distinct(side),
  n_cond = n_distinct(cond))

# What?
str(stan_data)
```


Define the new `model_code` and fit `m11.5`. Note how this time we're defining the log likelihood values in the `generated quantities` block with `binomial_lpmf()`.


```{r}
model_code_11.5 <- '
data {
  int<lower=1> n;                       
  int<lower=1> n_actor;
  int<lower=1> n_side;
  int<lower=1> n_cond;
  array[n] int actor;
  array[n] int side;
  array[n] int cond;
  array[n] int<lower=0, upper=1> pulled_left;
}
parameters {
  vector[n_actor] a;
  vector[n_side] b1;
  vector[n_cond] b2;
}
model {
  pulled_left ~ binomial(1, inv_logit(a[actor] + b1[side] + b2[cond]));
  a ~ normal(0, 1.5);
  b1 ~ normal(0, 0.5);
  b2 ~ normal(0, 0.5);
}
generated quantities {
  vector[n] eta;  // To simplify the `log_lik` code
  vector[n] log_lik;
  
  eta = a[actor] + b1[side] + b2[cond];
  for (i in 1:n) log_lik[i] = binomial_lpmf(pulled_left[i] | 1, inv_logit(eta[i]));
  
  // This also works for the `log_lik`, and it negates the need for `eta` from above
  // for (i in 1:n) log_lik[i] = binomial_lpmf(pulled_left[i] | 1, inv_logit(a[actor[i]] + b1[side[i]] + b2[cond[i]]));
}
'

m11.5 <- stan(
  model_code = model_code_11.5, 
  data = stan_data,
  cores = 4, seed = 11)
```


Check the model summary.


```{r}
print(m11.5, pars = c("a", "b1", "b2"), probs = c(0.055, 0.945))
```


Compare the two models by the LOO with the `extract_log_lik()` and `loo_compare()` functions.


```{r}
loo_compare(
  extract_log_lik(m11.4) |> loo(),
  extract_log_lik(m11.5) |> loo()
) |> 
  print(simplify = FALSE)
```


#### Overthinking: Adding log-probability calculations to a Stan model.

We did this with `m11.5`, above.

### Relative shark and absolute deer.

Based on the full model, `m11.4`, here's how you might compute the posterior mean and 89% intervals for the proportional odds of switching from `treatment == 2` to `treatment == 4`.


```{r, warning = FALSE}
as_draws_df(m11.4) |> 
  mutate(proportional_odds = exp(`b[4]` - `b[2]`)) |> 
  mean_qi(proportional_odds, .width = 0.89)
```


A limitation of relative measures measures like proportional odds is they ignore what you might think of as the reference or the baseline.

#### Overthinking: Proportional odds and relative risk.

### Aggregated binomial: Chimpanzees again, condensed.

With the **tidyverse**, we can use `group_by()` and `summarise()` to achieve what McElreath did with `aggregate()`.


```{r}
#| message: false

d_aggregated <- d |>
  group_by(treatment, actor, side, cond) |>
  summarise(left_pulls = sum(pulled_left)) |> 
  ungroup()

# What?
d_aggregated |>
  head(n = 10)
```


Update the `stan_data` for the new `d_aggregated` format.


```{r}
stan_data <- d_aggregated |> 
  compose_data(n_actor = n_distinct(d_aggregated$actor))

# What?
str(stan_data)
```


Define `model_code_11.6` and fit the model.


```{r}
model_code_11.6 <- '
data {
  int<lower=1> n;
  int<lower=1> n_actor;
  int<lower=1> n_treatment;
  array[n] int treatment;
  array[n] int actor;
  array[n] int<lower=0, upper=18> left_pulls;
}
parameters {
  vector[n_actor] a;
  vector[n_treatment] b;
}
model {
  left_pulls ~ binomial(18, inv_logit(a[actor] + b[treatment]));
  a ~ normal(0, 1.5);
  b ~ normal(0, 0.5);
}
generated quantities {
  vector[n] eta;
  vector[n] log_lik;
  eta = a[actor] + b[treatment];
  for (i in 1:n) log_lik[i] = binomial_lpmf(left_pulls[i] | 18, inv_logit(eta[i]));
}
'

m11.6 <- stan(
  model_code = model_code_11.6, 
  data = stan_data,
  cores = 4, seed = 11)
```


Rather than the typical `print()` summary, here we'll compare the `a` and `b` parameters in this aggregated binomial model `m11.6` with its disaggregated version `m11.4` in a coefficient plot.


```{r, fig.width = 8, fig.height = 2.5, warning = FALSE}
bind_rows(
  as_draws_df(m11.4) |> 
    select(.draw, `a[1]`:`b[4]`) ,
  as_draws_df(m11.6) |> 
    select(.draw, `a[1]`:`b[4]`) 
) |> 
  mutate(type = rep(c("disaggregated", "aggregated"), each = n() / 2)) |> 
  pivot_longer(`a[1]`:`b[4]`) |> 
  mutate(class = str_extract(name, "\\w")) |> 
  
  ggplot(aes(x = value, y = name,
             color = type)) +
  stat_pointinterval(.width = 0.89, linewidth = 1, shape = 1,
                     position = position_dodge(width = -0.5)) +
  scale_y_discrete(NULL, labels = ggplot2:::parse_safe) +
  xlab("posterior") +
  facet_wrap(~ class, scales = "free")
```


Different version of the likelihood, same model. Feed the `extract_log_lik()` results into `loo()`, and save output as `loo_m11.6`.


```{r}
loo_m11.6 <- extract_log_lik(m11.6) |>
  loo()

# What?
print(loo_m11.6)
```


Unlike with McElreath's `compare()` code in the text, the `loo::loo_compare()` will not let us directly compare the two versions of the model.


```{r}
loo_compare(
  extract_log_lik(m11.4) |> loo(),
  loo_m11.6
) |> 
  print(simplify = FALSE)
```


To understand what's going on, consider how you might describe six 1's out of nine trials in the aggregated form,

$$\Pr(6|9, p) = \frac{6!}{6!(9 - 6)!} p^6 (1 - p)^{9 - 6}.$$

If we still stick with the same data, but this time re-express those as nine dichotomous data points, we now describe their joint probability as

$$\Pr(1, 1, 1, 1, 1, 1, 0, 0, 0 | p) = p^6 (1 - p)^{9 - 6}.$$

Let's work this out in code.


```{r}
# deviance of aggregated 6-in-9 
-2 * dbinom(6, size = 9, prob = 0.2, log = TRUE)

# deviance of dis-aggregated 
-2 * sum(dbinom(c(1, 1, 1, 1, 1, 1, 0, 0, 0), size = 1, prob = 0.2, log = TRUE))
```


> But this difference is entirely meaningless. It is just a side effect of how we organized the data. The posterior distribution for the probability of success on each trial will end up the same, either way. (p. 339)

This is what our coefficient plot showed us, above. The posterior distribution was the same within simulation variance for `m11.4` and `m11.6`. Just like McElreath reported in the text, we also got a warning about high Pareto $k$ values from the aggregated binomial model, `m11.6`. To access the message and its associated table directly, we can feed our `loo_m11.6` object into the `loo::pareto_k_table` function.


```{r}
loo_m11.6 |> 
  loo::pareto_k_table()
```


### Aggregated binomial: Graduate school admissions.

Load the infamous `UCBadmit` data.


```{r}
data(UCBadmit, package = "rethinking")
d <- UCBadmit
rm(UCBadmit)

# What?
print(d)
```


Now compute our new index variable, `gid`. Notice that if we save `gid` as a factor, the `compose_data()` a couple blocks below will automatically compute `n_gid`. We'll also slip in a `case` variable that saves the row numbers as a factor, which will come in handy later when we plot.


```{r}
d <- d |>  
  mutate(gid  = ifelse(applicant.gender == "male", 1, 2) |> factor(),
         case = 1:n() |> factor())

# What?
d |> 
  distinct(applicant.gender, gid)
```


The univariable logistic model with `male` as the sole predictor of `admit` follows the form

$$
\begin{align*}
\text{admit}_i    & \sim \operatorname{Binomial}(n_i, p_i) \\
\text{logit}(p_i) & = \alpha_{\text{gid}[i]} \\
\alpha_j          & \sim \operatorname{Normal}(0, 1.5),
\end{align*}
$$

where $n_i = \text{applications}_i$, the rows are indexed by $i$, and the two levels of $\text{gid}$ are indexed by $j$. 

Make the `stan_data` with the `compose_data()` function.


```{r}
stan_data <- d |> 
  select(dept, gid, admit, applications) |>
  compose_data()

# What?
str(stan_data)
```


Make the `model_code` and fit `m11.7` with `stan()`. Note the `generated quantities` block for the posterior-predictive check to come.


```{r}
model_code_11.7 <- '
data {
  int<lower=1> n;
  int<lower=1> n_gid;
  array[n] int gid;
  array[n] int<lower=1> applications;
  array[n] int<lower=0> admit;
}
parameters {
  vector[n_gid] a;
}
model {
  admit ~ binomial(applications, inv_logit(a[gid]));
  a ~ normal(0, 1.5);
}
generated quantities {
  // For the pp check
  array[n] int<lower=0> pp_admit = binomial_rng(applications, inv_logit(a[gid]));
}
'

m11.7 <- stan(
  model_code = model_code_11.7, 
  data = stan_data,
  cores = 4, iter = 4000, seed = 11)
```


Check the model summary.


```{r}
print(m11.7, pars = "a", probs = c(0.055, 0.945))
```


We'll hold off on computing `diff_a` and `diff_p`, for a moment, and jump straight to Figure 11.5. Note how we're using the `pp_admit` values we computed with the `generated quantities` block.


```{r, fig.width = 6, fig.height = 3}
m11.7 |> 
  spread_draws(pp_admit[i]) |> 
  left_join(d |> 
              mutate(i = as.integer(case)),
            by = join_by(i)) |> 
  
  ggplot(aes(x = gid)) +
  stat_pointinterval(aes(y = pp_admit / applications),
                     .width = 0.89, linewidth = 1, shape = 1) +
  geom_point(data = d,
             aes(y = admit / applications),
             color = "blue") +
  geom_path(data = d,
             aes(y = admit / applications, group = dept),
             color = "blue") +
  scale_x_discrete(NULL, labels = c("male", "female")) +
  scale_y_continuous("admit", limits = 0:1) +
  facet_wrap(~ dept, nrow = 1)
```


I should acknowledge I got the idea to reformat the plot this way from Arel-Bundock's [work](https://vincentarelbundock.github.io/rethinking2/11.html).

Now we fit the model

$$
\begin{align*}
\text{admit}_i    & \sim \operatorname{Binomial} (n_i, p_i) \\
\text{logit}(p_i) & = \alpha_{\text{gid}[i]} + \delta_{\text{dept}[i]} \\
\alpha_j          & \sim \operatorname{Normal} (0, 1.5) \\
\delta_k          & \sim \operatorname{Normal} (0, 1.5),
\end{align*}
$$

where departments are indexed by $k$. Make the `model_code` and fit `m11.8` with `stan()`. Note the increase in the `iter` argument.


```{r}
model_code_11.8 <- '
data {
  int<lower=1> n;
  int<lower=1> n_gid;
  int<lower=1> n_dept;
  array[n] int gid;
  array[n] int dept;
  array[n] int applications;
  array[n] int<lower=0> admit;
}
parameters {
  vector[n_gid] a;
  vector[n_dept] d;
}
model {
  admit ~ binomial(applications, inv_logit(a[gid] + d[dept]));
  a ~ normal(0, 1.5);
  d ~ normal(0, 1.5);
}
generated quantities {
  // For the pp check
  array[n] int<lower=0> pp_admit = binomial_rng(applications, inv_logit(a[gid] + d[dept]));
}
'

m11.8 <- stan(
  model_code = model_code_11.8, 
  data = stan_data,
  cores = 4, iter = 4000, seed = 11)
```


Check the model summary.


```{r}
print(m11.8, pars = c("a", "d"), probs = c(0.055, 0.945))
```


Here we'll show the contrasts for the two models in a coefficient plot.


```{r, fig.width = 6, fig.height = 1.75, warning = F}
bind_rows(
  as_draws_df(m11.7) |> select(.draw, `a[1]`:`a[2]`),
  as_draws_df(m11.8) |> select(.draw, `a[1]`:`a[2]`)
) |> 
  mutate(fit = rep(c("m11.7", "m11.8"), each = n() / 2),
         diff_a = `a[1]` - `a[2]`,
         diff_p = plogis(`a[1]`) - plogis(`a[2]`)) |> 
  pivot_longer(contains("diff")) |> 
  
  ggplot(aes(x = value, y = name,
             color = fit)) +
  geom_vline(xintercept = 0, color = "white") +
  stat_pointinterval(.width = 0.89, linewidth = 1, shape = 1,
                     position = position_dodge(width = -0.5)) +
  labs(x = "contrast",
       y = NULL)
```


Here's our **tidyverse**-style tabulation of the proportions of applicants in each department by `gid`.


```{r}
#| message: false
#| warning: false

d |> 
  group_by(dept) |> 
  mutate(proportion = applications / sum(applications)) |> 
  select(dept, gid, proportion) |> 
  pivot_wider(names_from = dept,
              values_from = proportion) |> 
  mutate_if(is.double, round, digits = 2)
```


If we make another version of Figure 11.5 with `m11.8`, we'll see conditioning on both substantially improved the posterior predictive distribution.


```{r, fig.width = 6, fig.height = 3}
m11.8 |> 
  spread_draws(pp_admit[i]) |> 
  left_join(d |> 
              mutate(i = as.integer(case)),
            by = join_by(i)) |> 
  
  ggplot(aes(x = gid)) +
  stat_pointinterval(aes(y = pp_admit / applications),
                     .width = 0.89, linewidth = 1, shape = 1) +
  geom_point(data = d,
             aes(y = admit / applications),
             color = "blue") +
  geom_path(data = d,
             aes(y = admit / applications, group = dept),
             color = "blue") +
  scale_x_discrete(NULL, labels = c("male", "female")) +
  scale_y_continuous("admit", limits = 0:1) +
  facet_wrap(~ dept, nrow = 1)
```


McElreath recommended we look at the `pairs()` plot to get a sense of how highly correlated the parameters in our `m11.8` model are. Here's the plot.


```{r, fig.width = 5.5, fig.height = 5, warning = F}
pairs(m11.8, pars = c("a", "d"), gap = 0.25)
```


#### Rethinking: Simpson's paradox is not a paradox.

## Poisson regression

### Example: Oceanic tool complexity.

Load the `Kline` data [see @klinePopulationSizePredicts2010].


```{r}
data(Kline, package = "rethinking")
d <- Kline
rm(Kline)

# What?
print(d)
```


Here are our new columns.


```{r}
d <- d |>
  mutate(log_pop_std = (log(population) - mean(log(population))) / sd(log(population)),
         cid         = ifelse(contact == "high", "2", "1"))
```


Our statistical model will follow the form

$$
\begin{align*}
\text{total_tools}_i & \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i)      & = \alpha_{\text{cid}[i]} + \beta_{\text{cid}[i]} \text{log_pop_std}_i \\
\alpha_j             & \sim \; ? \\
\beta_j              & \sim \; ?, 
\end{align*}
$$

where the priors for $\alpha_j$ and $\beta_j$ have yet be defined. If we continue our convention of using a Normal prior on the $\alpha$ parameters, we should recognize those will be log-Normal distributed on the outcome scale. Why? Because we're modeling $\lambda$ with the log link. Here's our version of Figure 11.7, depicting the two log-Normal priors considered in the text.


```{r, fig.width = 4, fig.height = 2.75}
d_plot <- tibble(
  x       = c(3, 22),
  y       = c(0.055, 0.04),
  meanlog = c(0, 3),
  sdlog   = c(10, 0.5)) |> 
  expand_grid(number = seq(from = 0, to = 100, length.out = 200)) |> 
  mutate(density = dlnorm(number, meanlog, sdlog),
         group   = str_c("alpha%~%Normal(", meanlog, ", ", sdlog, ")"))

d_plot |> 
  ggplot(aes(fill = group, color = group)) +
  geom_area(aes(x = number, y = density),
            alpha = 3/4, linewidth = 0, position = "identity") +
  geom_text(data = d_plot |> 
              group_by(group) |> 
              slice(1),
            aes(x = x, y = y, label = group),
            hjust = 0, parse = TRUE) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("mean number of tools") +
  theme(legend.position = "none")
```


In this context, $\alpha \sim \operatorname{Normal}(0, 10)$ has a very long tail on the outcome scale. The mean of the log-Normal distribution, recall, is $\exp (\mu + \sigma^2/2)$. Here that is in code.


```{r}
exp(0 + 10^2 / 2)
```


That is very large. Here's the same thing in a simulation.


```{r}
set.seed(11)

rnorm(1e4, 0, 10) |> 
  exp() |> 
  mean()
```


Now compute the mean for the other prior under consideration, $\alpha \sim \operatorname{Normal}(3, 0.5)$.


```{r}
exp(3 + 0.5^2 / 2)
```


This is much smaller and more reasonable.

Now let's prepare to make the top row of Figure 11.8. In this portion of the figure, we consider the implications of two competing priors for $\beta$ while holding the prior for $\alpha$ at $\operatorname{Normal}(3, 0.5)$. The two $\beta$ priors under consideration are $\operatorname{Normal}(0, 10)$ and $\operatorname{Normal}(0, 0.2)$.


```{r, fig.width = 6, fig.height = 3}
set.seed(11)

# How many lines would you like?
n <- 100

# Simulate and wrangle
tibble(i = 1:n,
       a = rnorm(n, mean = 3, sd = 0.5)) |> 
  mutate(`beta%~%Normal(0*', '*10)`  = rnorm(n, mean = 0 , sd = 10),
         `beta%~%Normal(0*', '*0.2)` = rnorm(n, mean = 0 , sd = 0.2)) |> 
  pivot_longer(contains("beta"),
               values_to = "b",
               names_to = "prior") |> 
  expand_grid(x = seq(from = -2, to = 2, length.out = 100)) |> 
  mutate(prior = fct_rev(prior)) |> 
  
  # Plot
  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +
  geom_line(alpha = 2/3, linewidth = 1/4) +
  labs(x = "log population (std)",
       y = "total tools") +
  coord_cartesian(ylim = c(0, 100)) +
  facet_wrap(~ prior, labeller = label_parsed)
```


It turns out that many of the lines considered plausible under $\operatorname{Normal}(0, 10)$ are disturbingly extreme. Here is what $\alpha \sim \operatorname{Normal}(3, 0.5)$ and $\beta \sim \operatorname{Normal}(0, 0.2)$ would mean when the $x$-axis is on the log population scale and the population scale.


```{r, fig.width = 6, fig.height = 3.2}
set.seed(11)

prior <- tibble(
  i = 1:n,
  a = rnorm(n, mean = 3, sd = 0.5),
  b = rnorm(n, mean = 0, sd = 0.2)) |> 
  expand_grid(x = seq(from = log(100), to = log(200000), length.out = 100))

# Left
p1 <- prior |> 
  ggplot(aes(x = x, y = exp(a + b * x), group = i)) +
  geom_line(alpha = 2/3, linewidth = 1/4) +
  labs(x = "log population",
       y = "total tools") +
  coord_cartesian(xlim = c(log(100), log(200000)),
                  ylim = c(0, 500))
# Right
p2 <- prior |> 
  ggplot(aes(x = exp(x), y = exp(a + b * x), group = i)) +
  geom_line(alpha = 2/3, linewidth = 1/4) +
  scale_y_continuous(NULL, breaks = NULL) +
  xlab("population") +
  coord_cartesian(xlim = c(100, 200000),
                  ylim = c(0, 500))

# Combine, add facet strips, and display
(p1 | p2) &
  facet_wrap(~ "atop(alpha%~%Normal(3*', '*0.5), beta%~%Normal(0*', '*0.2))", 
             labeller = label_parsed)
```


Okay, after settling on our two priors, the updated model formula is

$$
\begin{align*}
y_i             & \sim \operatorname{Poisson}(\lambda_i) \\
\log(\lambda_i) & = \alpha + \beta (x_i - \bar x) \\
\alpha          & \sim \operatorname{Normal}(3, 0.5) \\
\beta           & \sim \operatorname{Normal}(0, 0.2).
\end{align*}
$$

We're finally ready to start fitting the models. First, define the `stan_data`.


```{r}
stan_data <- d |> 
  select(total_tools, log_pop_std, cid) |> 
  compose_data()

# What?
str(stan_data)
```

```{r}
# Intercept only
model_code_11.9 <- '
data {
  int<lower=1> n;
  array[n] int<lower=0> total_tools;
}
parameters {
  real a;  
}
model {
  total_tools ~ poisson(exp(a));
  a ~ normal(3, 0.5);
}
generated quantities {
  vector[n] log_lik;
  for (i in 1:n) log_lik[i] = poisson_lpmf(total_tools[i] | exp(a));
}
'

# Interaction model
model_code_11.10 <- '
data {
  int<lower=1> n;
  int<lower=1> n_cid;
  array[n] int cid;
  vector[n] log_pop_std;
  array[n] int<lower=0> total_tools;
}
parameters {
  vector[n_cid] a;
  vector[n_cid] b;
}
model {
  vector[n] lambda;
  lambda = exp(a[cid] + b[cid] .* log_pop_std);
  
  total_tools ~ poisson(lambda);
  a ~ normal(3, 0.5);
  b ~ normal(0, 0.2);
}
generated quantities {
  vector[n] log_lik;
  for (i in 1:n) log_lik[i] = poisson_lpmf(total_tools[i] | exp(a[cid[i]] + b[cid[i]] .* log_pop_std[i]));
}
'
```


Sample from the posteriors.


```{r}
m11.9 <- stan(
  data = stan_data,
  model_code = model_code_11.9,
  cores = 4, seed = 11)

m11.10 <- stan(
  data = stan_data,
  model_code = model_code_11.10,
  cores = 4, seed = 11)
```


Compare the two models by the LOO.


```{r}
l11.9 <- extract_log_lik(m11.9) |> loo()
l11.10 <- extract_log_lik(m11.10) |> loo()

loo_compare(l11.9, l11.10) |> 
  print(simplify = FALSE)
```


Like McElreath reported in the text, we have a Pareto-k warning. We can inspect the Pareto $k$ values with `loo::pareto_k_table()`.


```{r}
pareto_k_table(l11.10)
```


Let's take a closer look.


```{r}
d |> 
  select(culture) |> 
  mutate(k = l11.10$diagnostics$pareto_k) |> 
  arrange(desc(k)) |> 
  mutate_if(is.double, round, digits = 2)
```


It turns out Hawaii is very influential. Figure 11.9 will clarify why. For a little practice while computing the trajectories, we'll use an `as_draws_df()`-based workflow for the panel on the left, and use a  `spread_draws()`-based workflow for the panel on the right.


```{r, fig.width = 7, fig.height = 3.25, warning = F}
# For subsetting the labels in the left panel
culture_vec <- c("Hawaii", "Tonga", "Trobriand", "Yap")

# Left
p1 <- as_draws_df(m11.10) |> 
  select(.draw, `a[1]`:`b[2]`) |> 
  pivot_longer(-.draw) |> 
  mutate(parameter = str_extract(name, "[a-z]"),
         cid = str_extract(name, "\\d")) |> 
  select(-name) |> 
  pivot_wider(names_from = parameter, values_from = value) |> 
  expand_grid(log_pop_std = seq(from = -4.5, to = 2.5, length.out = 100)) |> 
  mutate(total_tools = exp(a + b * log_pop_std)) |> 
  
  ggplot(aes(x = log_pop_std, y = total_tools,
             group = cid, color = cid, fill = cid)) +
  stat_lineribbon(.width = 0.89, alpha = 1/4) +
  geom_point(data = d |> 
               mutate(k = l11.10$diagnostics$pareto_k),
             aes(size = k)) +
  geom_text(data = d |> 
              mutate(k = l11.10$diagnostics$pareto_k) |> 
              mutate(label = str_c(culture, " (", round(k, digits = 2), ")"),
                     vjust = ifelse(total_tools > 30, -0.3, 1.3)) |> 
              filter(culture %in% culture_vec),
            aes(label = label, vjust = vjust),
            hjust = 1.1, size = 3) +
  labs(x = "log population (std)",
       y = "total tools") +
  coord_cartesian(xlim = range(d$log_pop_std),
                  ylim = c(0, 80))

# Right
p2 <- spread_draws(m11.10, a[cid], b[cid]) |> 
  mutate(cid = as.character(cid)) |> 
  expand_grid(log_pop_std = seq(from = -4.5, to = 2.5, length.out = 100)) |> 
  mutate(total_tools = exp(a + b * log_pop_std)) |>
  mutate(population = exp((log_pop_std * sd(log(d$population))) + mean(log(d$population)))) |> 
  
  ggplot(aes(x = population, y = total_tools,
             group = cid, color = cid, fill = cid)) +
  stat_lineribbon(.width = 0.89, alpha = 1/4) +
  geom_point(data = d |> 
               mutate(k = l11.10$diagnostics$pareto_k),
             aes(size = k)) +
  scale_x_continuous("population", breaks = c(0, 50000, 150000, 250000),
                     labels = scales::comma(c(0, 50000, 150000, 250000))) +
  scale_y_continuous(NULL, breaks = NULL) +
  coord_cartesian(xlim = range(d$population),
                  ylim = c(0, 80))

# Combine, adjust, and display
(p1 | p2) &
  scale_color_viridis_d(option = "A", end = 0.6) &
  scale_fill_viridis_d(option = "A", end = 0.6) &
  scale_size_continuous(range = c(0.1, 5), limits = c(0.1, 1.1)) &
  theme(legend.position = "none")
```


Hawaii is influential in that it has a very large population relative to the other islands.

#### Overthinking: Modeling tool innovation.

McElreath's theoretical, or scientific, model for `total_tools` is

$$\widehat{\text{total_tools}} = \frac{\alpha_{\text{cid}[i]} \: \text{population}^{\beta_{\text{cid}[i]}}}{\gamma}.$$

We can use the Poisson likelihood to express this in a Bayesian model as

$$
\begin{align*}
\text{total_tools} & \sim \operatorname{Poisson}(\lambda_i) \\
\lambda_i & = \left[ \exp (\alpha_{\text{cid}[i]}) \text{population}_i^{\beta_{\text{cid}[i]}} \right] / \gamma \\
\alpha_j  & \sim \operatorname{Normal}(1, 1) \\
\beta_j   & \sim \operatorname{Exponential}(1) \\
\gamma    & \sim \operatorname{Exponential}(1),
\end{align*}
$$

where we exponentiate $\alpha_{\text{cid}[i]}$ to restrict the posterior to zero and above. Here's how we might fit that model with **rstan**.

First we make some data for the predictions we'll display in the plot.


```{r}
d_pred <- tibble(
  population = seq(from = 0, to = 3e5, length.out = 101) |> 
    as.integer()) |> 
  expand_grid(cid = 1:2) |> 
  mutate(i = 1:n())

# What?
glimpse(d_pred)
```


Now make the `stan_data`.


```{r}
stan_data <- d |> 
  # mutate(population = as.double(population)) |> 
  select(total_tools, population, cid) |>
  compose_data(population_pred = d_pred$population,
               cid_pred = d_pred$cid,
               n_pred = nrow(d_pred))

# What?
str(stan_data)
```


Define `model_code_11.11`.


```{r}
model_code_11.11 <- '
data {
  int<lower=1> n;
  int<lower=1> n_cid;
  array[n] int cid;
  vector[n] population;
  array[n] int<lower=0> total_tools;
  // For predictions
  int<lower=1> n_pred;
  array[n_pred] int cid_pred;
  vector[n_pred] population_pred;
}
parameters {
  vector[n_cid] a;
  vector<lower=0>[n_cid] b;
  real<lower=0> g;
}
transformed parameters {
  vector[n] lambda;
  lambda = exp(a[cid]) .* population^b[cid] / g;
}
model {
  total_tools ~ poisson(lambda);
  a ~ normal(1, 1);
  b ~ exponential(1);
  g ~ exponential(1);
}
generated quantities {
  vector[n] log_lik;
  vector[n_pred] lambda_pred;
  
  for (i in 1:n) log_lik[i] = poisson_lpmf(total_tools[i] | lambda[i]);
  lambda_pred = exp(a[cid_pred]) .* population_pred^b[cid_pred] / g;
}
'
```


Sample from the posteriors.


```{r}
m11.11 <- stan(
  data = stan_data,
  model_code = model_code_11.11,
  cores = 4, seed = 11)
```


Check the model summary.


```{r}
print(m11.11, pars = c("a", "b", "g"), probs = c(0.055, 0.945))
```


Compute and check the PSIS-LOO estimates along with their diagnostic Pareto-$k$ values.


```{r}
l11.11 <- extract_log_lik(m11.11) |> loo()

print(l11.11)
```


We still have one high Pareto-$k$ value. Recall that due to the very small sample size, this isn't entirely surprising. If you're curious, here's a scatter plot of the Pareto-$k$ values for this model and the last.


```{r, fig.width = 3, fig.height = 3}
tibble(m11.10 = l11.10$diagnostics$pareto_k,
       m11.11 = l11.11$diagnostics$pareto_k) |> 
  
  ggplot(aes(x = m11.10, y = m11.11)) +
  geom_abline(color = "white") +
  geom_point() +
  scale_x_continuous(breaks = 0:5 / 5 + 0.1, limits = c(0.1, 1.1)) +
  scale_y_continuous(breaks = 0:5 / 5 + 0.1, limits = c(0.1, 1.1)) +
  labs(subtitle = expression(Pareto~italic(k)))
```


Okay, it's time to make Figure 11.10. In the first line in the `Wrangle` section, note how we're using `spread_draws()` to extract the `lambda_pred` values we defined in the `generated quantities`, above.


```{r, fig.width = 3.5, fig.height = 3.25}
# For the annotation
d_text <- distinct(d, cid, contact) |> 
  mutate(population  = c(210000, 72500),
         total_tools = c(59, 68),
         label       = str_c(contact, " contact"))

# Wrangle
spread_draws(m11.11, lambda_pred[i]) |> 
  left_join(d_pred, by = join_by(i)) |> 
  mutate(cid = as.character(cid)) |> 
  rename(total_tools = lambda_pred) |> 
  
  # Plot!
  ggplot(aes(x = population, y = total_tools,
             group = cid, color = cid, fill = cid)) +
  stat_lineribbon(.width = 0.89, alpha = 1/4) +
  geom_point(data = d |> 
               mutate(k = l11.11$diagnostics$pareto_k),
             aes(size = k)) +
  geom_text(data = d_text,
            aes(label = label)) +
  scale_x_continuous("population", breaks = c(0, 50000, 150000, 250000),
                     labels = scales::comma(c(0, 50000, 150000, 250000))) +
  scale_color_viridis_d(option = "A", end = 0.6) +
  scale_fill_viridis_d(option = "A", end = 0.6) +
  scale_size_continuous(range = c(0.1, 5), limits = c(0.1, 1)) +
  ylab("total tools") +
  coord_cartesian(xlim = range(d$population),
                  ylim = c(0, 80)) +
  theme(legend.position = "none")
```


In case you were curious, here are the results if we compare `m11.10` and `m11.11` by the PSIS-LOO.


```{r}
loo_compare(l11.10, l11.11) |> 
  print(simplify = FALSE)
```


### Negative binomial (gamma-Poisson) models.

### Example: Exposure and the offset.

Here we simulate our data.


```{r}
set.seed(11)

num_days <- 30
y        <- rpois(num_days, lambda = 1.5)

num_weeks <- 4
y_new     <- rpois(num_weeks, lambda = 0.5 * 7)
```


Now tidy the data and add `log_days`.


```{r}
d <- tibble(
  y         = c(y, y_new), 
  days      = rep(c(1, 7), times = c(num_days, num_weeks)),  # this is the exposure
  monastery = rep(0:1, times = c(num_days, num_weeks))) |>
  mutate(log_days = log(days))

# What?
glimpse(d)
```


Within the context of the Poisson likelihood, we can decompose $\lambda$ into two parts, $\mu$ (mean) and $\tau$ (exposure), like this:

$$
y_i \sim \operatorname{Poisson}(\lambda_i) \\
\log \lambda_i = \log \frac{\mu_i}{\tau_i} = \log \mu_i - \log \tau_i.
$$

Therefore, you can rewrite the equation if the exposure ($\tau$) varies in your data and you still want to model the mean ($\mu$). Using the model we're about to fit as an example, here's what that might look like:

$$
\begin{align*}
y_i & \sim \operatorname{Poisson}(\mu_i) \\
\log \mu_i & = \color{blue}{\log \tau_i} + \alpha + \beta \text{monastery}_i \\
\alpha     & \sim \operatorname{Normal}(0, 1) \\
\beta      & \sim \operatorname{Normal}(0, 1),
\end{align*}
$$

where the offset $\log \tau_i$ does not get a prior. In this context, its value is added directly to the right side of the formula. With the **rstan** package, we do this directly in the `model` block. First, though, let's make the `stan_data`.


```{r}
stan_data <- d |> 
  compose_data()

# What?
str(stan_data)
```


Define `model_code_11.12`. Note how `log_days` does not get a coefficient in the `lambda` formula. It stands on its own.


```{r}
model_code_11.12 <- '
data {
  int<lower=1> n;
  vector[n] log_days;
  vector[n] monastery;
  array[n] int<lower=0> y;
}
parameters {
  real a;
  real b;
}
model {
  y ~ poisson(exp(log_days + a + b * monastery));
  a ~ normal(0, 1);
  b ~ normal(0, 1);
}
'
```


Sample from the posterior.


```{r}
m11.12 <- stan(
  data = stan_data,
  model_code = model_code_11.12,
  cores = 4, seed = 11)
```


As we look at the model summary, keep in mind that the parameters are on the per-one-unit-of-time scale. Since we simulated the data based on summary information from two units of time--one day and seven days--, this means the parameters are in the scale of $\log (\lambda)$ per *one* day.


```{r}
print(m11.12, probs = c(0.055, 0.945))
```


To get the posterior distributions for average daily outputs for the old and new monasteries, respectively, we'll use use the formulas

$$
\begin{align*}
\lambda_\text{old} & = \exp (\alpha) \;\;\; \text{and} \\
\lambda_\text{new} & = \exp (\alpha + \beta_\text{monastery}).
\end{align*}
$$

Following those transformations, we'll summarize the $\lambda$ distributions with medians and 89% HDIs with help from the `tidybayes::mean_hdi()` function.


```{r, warning = F}
as_draws_df(m11.12) |>
  mutate(lambda_old = exp(a),
         lambda_new = exp(a + b)) |>
  pivot_longer(contains("lambda")) |> 
  mutate(name = factor(name, levels = c("lambda_old", "lambda_new"))) |>
  group_by(name) |>
  mean_hdi(value, .width = .89) |> 
  mutate_if(is.double, round, digits = 2)
```


Because we don't know what seed McElreath used to simulate his data, our simulated data differed a little from his and, as a consequence, our results differ a little, too.

## Multinomial and categorical models

### Predictors matched to outcomes.

To begin, let's simulate the data just like McElreath did in the **R** code 11.55 block.


```{r}
#| message: false
#| warning: false

library(rethinking)

# simulate career choices among 500 individuals
n      <- 500           # number of individuals
income <- c(1, 2, 5)    # expected income of each career
score  <- 0.5 * income  # scores for each career, based on income

# next line converts scores to probabilities
p <- softmax(score[1], score[2], score[3])

# now simulate choice
# outcome career holds event type values, not counts
career <- rep(NA, n)  # empty vector of choices for each individual

# sample chosen career for each individual
set.seed(34302)
# sample chosen career for each individual
for(i in 1:n) career[i] <- sample(1:3, size = 1, prob = p)
```


Before moving on, it might be useful to examine what we just did. With the three lines below the "# simulate career choices among 500 individuals" comment, we defined the formulas for three scores. Those were

$$
\begin{align*}
s_1 & = 0.5 \times \text{income}_1 \\
s_2 & = 0.5 \times \text{income}_2 \\ 
s_3 & = 0.5 \times \text{income}_3,
\end{align*}
$$

where $\text{income}_1 = 1$, $\text{income}_2 = 2$, and $\text{income}_3 = 5$. What's a little odd about this setup and conceptually important to get is that although $\text{income}_i$ varies across the three levels of $s$, the $\text{income}_i$ value is constant within each level of $s$. E.g., $\text{income}_1$ is not a variable within the context of $s_1$. Therefore, we could also write the above as

$$
\begin{align*}
s_1 & = 0.5 \cdot 1 = 0.5 \\
s_2 & = 0.5 \cdot 2 = 1.0 \\ 
s_3 & = 0.5 \cdot 5 = 2.5.
\end{align*}
$$

Let's confirm.


```{r}
print(score)
```


We then converted those `score` values to probabilities with the `softmax()` function. This will become important when we set up the model code. For now, here's what the data look like.


```{r, fig.width = 3, fig.height = 2.25}
# Put them in a tibble
d <- tibble(career = career) |> 
  mutate(career_income = ifelse(career == 3, 5, career) |> as.integer())

# plot 
d |>
  ggplot(aes(x = career)) +
  geom_bar(linewidth = 0)
```


Our `career` variable is composed of three categories, `1:3`, with each category more likely than the one before. Here's a breakdown of the counts, percentages, and probabilities of each category.


```{r}
d |> 
  count(career) |> 
  mutate(percent     = (100 * n / sum(n)),
         probability =        n / sum(n))
```


To further build an appreciation for how we simulated data with these proportions and how the process links in with the formulas, above, we'll retrace the first few simulation steps within a **tidyverse**-centric workflow. Recall how in those first few steps we defined values for `income`, `score`, and `p`. Here they are again in a tibble.


```{r}
tibble(income = c(1, 2, 5)) |> 
  mutate(score = 0.5 * income) |> 
  mutate(p = exp(score) / sum(exp(score)))
```


Notice how the values in the `p` column match up well with the `probability` values from the output from the block just above. Our simulation successfully produces data corresponding to the data-generating values. Also note how the code we just used to compute those `p` values, `p = exp(score) / sum(exp(score))`, corresponds nicely with the formula from above,

$$\Pr (k |s_1, s_2, \dots, s_K) = \frac{\exp (s_k)}{\sum_{i = 1}^K \exp (s_i)}.$$

What still might seem mysterious is what those $s$ values in the equation are. In the simulation and in the prose, McElreath called them *scores*. Another way to think about them is as weights. The thing to get is that their exact values aren't important so much as their difference one from another. You'll note that `score` for `income == 2` was 0.5 larger than that of `income == 1`. The same was true for `income == 3` and `income == 2`. So if we add an arbitrary constant to each of those `score` values, like 11, we'll get the same `p` values.


```{r}
tibble(income = c(1, 2, 5), 
       some_constant = 11) |> 
  mutate(score = (0.5 * income) + some_constant) |> 
  mutate(p = exp(score) / sum(exp(score)))
```


Now keeping that in mind, recall how McElreath said that though we have $K$ categories, $K = 3$ in this case, we only estimate $K - 1$ linear models. "In a multinomial (or categorical) GLM, you need $K - 1$ linear models for $K$ types of events. One of the outcome values is chosen as a 'pivot' and the others are modeled relative to it." (p. 360). You could also think of the pivot category as the reference category.

Okay, let's make the `stan_data`.


```{r}
n_distinct(d, career)
```

```{r}
stan_data <- d |> 
  select(career) |> 
  compose_data(n_careers = n_distinct(d, career),
               income = income)

# What?
str(stan_data)
```

```{r}
model_code_11.13 <- '
data {
  int<lower=1> n;
  int<lower=1> n_career;   // McElreath called this K
  vector[n_career] income;
  int career[n];
}
parameters {
  vector[n_career - 1] a;
  real<lower=0> b;         // Note the lower bound
}
model {
  vector[n_career] p;
  vector[n_career] s;
  s[1] = a[1] + b * income[1]; 
  s[2] = a[2] + b * income[2]; 
  s[3] = 0;                     // The reference category (i.e., pivot)
  p = softmax(s);
  career ~ categorical(p);
  
  a ~ normal(0, 1);
  b ~ normal(0, 0.5);  // Because of the lower bound, this is a half-Normal prior
} 
'
```


Sample from the posterior.






```{r}
m11.13 <- stan(
  data = stan_data,
  model_code = model_code_11.13,
  cores = 4, seed = 11)
```

```{r, eval = F}
# define the model
code_m11.13 <- "
data{
  int N; // number of individuals
  int K; // number of possible careers 
  int career[N]; // outcome
  vector[K] career_income;
}
parameters{
  vector[K - 1] a; // intercepts
  real<lower=0> b; // association of income with choice
}
model{
  vector[K] p;
  vector[K] s;
  a ~ normal(0, 1);
  b ~ normal(0, 0.5);
  s[1] = a[1] + b * career_income[1]; 
  s[2] = a[2] + b * career_income[2]; 
  s[3] = 0; // pivot
  p = softmax(s);
  career ~ categorical(p);
} 
"

# wrangle the data
dat_list <- 
  list(N = n, 
       K = 3, 
       career = career, 
       career_income = income)

# fit the model
m11.13 <- 
  stan(data = dat_list,
       model_code = code_m11.13,
       chains = 4)
```


Check the summary.


```{r}
precis(m11.13, depth = 2) |> round(digits = 2)
```


One of the primary reasons we went through this exercise is to show that McElreath's **R** code 11.56 and 11.57 do not return the results he reported on page 361. The plot thickens when we attempt the counterfactual simulation on page 362, as reported in **R** code 11.58.


```{r}
post <- extract.samples(m11.13)

# set up logit scores
s1      <- with(post, a[, 1] + b * income[1])
s2_orig <- with(post, a[, 2] + b * income[2])
s2_new  <- with(post, a[, 2] + b * income[2] * 2)

# compute probabilities for original and counterfactual 
p_orig <- sapply(1:length(post$b), function(i)
  softmax(c(s1[i], s2_orig[i], 0)))

p_new <- sapply(1:length(post$b), function(i)
  softmax(c(s1[i], s2_new[i], 0)))

# summarize
p_diff <- p_new[2, ] - p_orig[2, ] 
precis(p_diff)
```


Even though we used the same code, our counterfactual simulation doesn't match up with the results McElreath reported in the text, either. Keep this all in mind as we switch to **brms**. But before we move on to **brms**, check this out.


```{r}
data.frame(s1 = score[3] + s1, 
           s2 = score[3] + s2_orig, 
           s3 = score[3] + 0) |> 
  pivot_longer(everything()) |> 
  group_by(name) |> 
  mean_qi(value) |> 
  mutate_if(is.double, round, digits = 2)
```


In his Stan code (**R** code 11.56), you'll see McElreath chose the third category to be his pivot and that he used zero as a constant value. As it turns out, it is common practice to set the score value for the reference category to zero. It's also a common practice to use the first event type as the reference category. Importantly, in his [-@BÃ¼rkner2022Parameterization] vignette, [*Parameterization of response distributions in brms*](https://cran.r-project.org/package=brms/vignettes/brms_families.html#ordinal-and-categorical-models), BÃ¼rkner clarified the **brms** default is to use the first response category as the reference and set it to a zero as well. However, we can control this behavior with the `refcat` argument. In the examples to follow, we'll follow McElreath and use the third event type as the reference category by setting `refcat = 3`.

In addition to the discrepancies with the code and results in the text, one of the things I don't care for in this section is how fast McElreath covered the material. Our approach will be to slow down a little and start off by fitting a intercepts-only model before adding the covariate. Before we fit the model, we might take a quick look at the prior structure with `brms::get_prior()`.


```{r}
get_prior(data = d, 
          family = categorical(link = logit, refcat = 3),
          career ~ 1)
```


We have two "intercepts", which are differentiated in the `dpar` column. We'll talk more about what these are in just a bit; don't worry. I show this here because as of **brms** 2.12.0, "specifying global priors for regression coefficients in categorical models is deprecated." The upshot is even if we want to use the same prior for both, we need to use the `dpar` argument for each. With that in mind, here's our multinomial model in **brms**. Do note the specification `family = categorical(link = logit, refcat = 3)`. The `categorical` part is what instructs **brms** to use the multinomial likelihood and the `refcat = 3` part will allow us to use the third event type as the pivot.


```{r}
m11.13io <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      career ~ 1,
      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),
                prior(normal(0, 1), class = Intercept, dpar = mu2)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      file = "fits/m11.13io")
```


The summary can be difficult to interpret.


```{r}
print(m11.13io)
```


`brms::brm()` referred to the $K$ categories as `mu1`, `mu2`, and `mu3`. Since `career == 3` is the reference category, the score for which was set to zero, there is no parameter for `mu3_Intercept`. That's a zero. Now notice how `mu1_Intercept` is about -2 and `mu2_Intercept` is about -1.5. If we double back to the `income` and `score` values we played with at the beginning of this section, you'll notice that the score for the reference category was 2.5. Here's what happens if we rescale the three scores such that the `score` value for the reference category is 0.


```{r}
tibble(income = c(1, 2, 5)) |> 
  mutate(score = 0.5 * income) |> 
  mutate(rescaled_score = score - 2.5)
```


Now notice how the `rescaled_score` values for the first two rows correspond nicely to `mu1_Intercept` and `mu2_Intercept` from our model. What I hope this clarifies is that our statistical model returned the scores. But recall these are not quite probabilities. *Why?* Because the weights are all relative to one another. The easiest way to get what we want, the probabilities for the three categories, is with `brms::fitted()`. Since this model has no predictors, only intercepts, we won't specify any `newdata`. In such a case, `fitted()` will return fitted values for each case in the data. Going slow, let's take a look at the structure of the output.


```{r}
fitted(m11.13io) |> str()
```


Just as expected, we have 500 rows--one for each case in the original data. We have four summary columns, the typical `Estimate`, `Est.Error`, `Q2.5`, and `Q97.5`. We also have third dimension composed of three levels, `P(Y = 1)`, `P(Y = 2)`, and `P(Y = 3)`. Those index which of the three career categories each probability summary is for. Since the results are identical for each row, we'll simplify the output by only keeping the first row.


```{r}
fitted(m11.13io)[1, , ] |> 
  round(digits = 2)
```


If we take the transpose of that, it will put the results in the format to which we've become accustomed.


```{r}
fitted(m11.13io)[1, , ] |> 
  round(digits = 2) |> 
  t()
```


Now compare those summaries with the empirically-derived percent and probability values we computed earlier.


```{r}
tibble(income = c(1, 2, 5)) |> 
  mutate(score = 0.5 * income) |> 
  mutate(p = exp(score) / sum(exp(score)))
```


Now here's how to make use of the formula from the last `mutate()` line, $\frac{\exp (s_k)}{\sum_{i = 1}^K \exp (s_i)}$, to compute the marginal probabilities from `m11.13io` by hand.


```{r, warning = F}
as_draws_df(m11.13io) |> 
  mutate(b_mu3_Intercept = 0) |> 
  mutate(p1 = exp(b_mu1_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),
         p2 = exp(b_mu2_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept)),
         p3 = exp(b_mu3_Intercept) / (exp(b_mu1_Intercept) + exp(b_mu2_Intercept) + exp(b_mu3_Intercept))) |> 
  pivot_longer(p1:p3) |> 
  group_by(name) |> 
  mean_qi(value) |> 
  mutate_if(is.double, round, digits = 2)
```


Hurray; we did it! Not only did we fit a simple multinomial model with **brms**, we actually made sense of the parameters by connecting them to the original data-generating values. We're almost ready to contend with the model McElreath fit with `stan()`. But before we do, it'll be helpful to show alternative ways to fit these models. We used conventional style syntax when we fit `m11.13io`. There are at least two alternative ways to fit the model:


```{r, eval = F}
# verbose syntax
m11.13io_verbose <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         mu1 ~ 1,
         mu2 ~ 1),
      prior = c(prior(normal(0, 1), class = Intercept, dpar = mu1),
                prior(normal(0, 1), class = Intercept, dpar = mu2)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      file = "fits/m11.13io_verbose")

# nonlinear syntax
m11.13io_nonlinear <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1),
         nlf(mu2 ~ a2),
         a1 + a2 ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      file = "fits/m11.13io_nonlinear")
```

```{r, eval = F, echo = F}
# not shown for the sake of space
print(m11.13io)
print(m11.13io_verbose)
print(m11.13io_nonlinear)
```


For the sake of space, I'm not going to show the results for those two models. If you fit them yourself, you'll see the results for `m11.13io` and `m11.13io_verbose` are exactly the same and `m11.13io_nonlinear` differs from them only within simulation variation. I point this out because it's the nonlinear approach that will allow us to fit a model like McElreath's `m11.13`. My hope is the syntax we used in the `m11.13io_verbose` model will help clarify what's going on with the non-linear syntax. When we fit multinomial models with **brms**, the terse conventional `formula` syntax might not make clear how there are actually $K - 1$ formulas. The more verbose syntax of our `m11.13io_verbose` model shows how we can specify those models directly. In our case, that was with those `mu1 ~ 1, mu2 ~ 1` lines. Had we used the **brms** default and used the first level of `career` as the pivot, those lines would have instead been `mu2 ~ 1, mu3 ~ 1`. So anyway, when we switch to the non-linear syntax, we explicitly model `mu1` and `mu2` and, as is typical of the non-linear syntax, we name our parameters. You can see another comparison of these three ways of fitting a multinomial model at the [Nonlinear syntax with a multinomial model?](https://discourse.mc-stan.org/t/nonlinear-syntax-with-a-multinomial-model/16122) thread on the Stan Forums.

Now it's time to focus on the **brms** version of McElreath's `m11.13`. To my eye, McElreath's model has two odd features. First, though he has two intercepts, he only has one $\beta$ parameter. Second, if you look at McElreath's `parameters` block, you'll see that he restricted his $\beta$ parameter to be zero and above (`real<lower=0> b;`).

With the **brms** non-linear syntax, we can fit the model with one $\beta$ parameter or allow the one $\beta$ parameter to differ for `mu1` and `mu2`. As to setting a lower bound to the `b` parameter[s], we can do that with the `lb` argument within the `prior()` function. If we fit our version of `m11.13` by systemically varying these two features, we'll end up with the four versions listed in the table below.


```{r}
crossing(b  = factor(c("b1 & b2", "b"), levels = c("b1 & b2", "b")),
         lb = factor(c("NA", 0), levels = c("NA", 0))) |> 
  mutate(fit = str_c("m11.13", letters[1:n()])) |> 
  select(fit, everything()) |> 
  
  flextable() |> 
  width(width = 1.25)
```


Fit `m11.13a` through `m11.13d`, the four variants on the model.


```{r m11.13a}
m11.13a <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b1 * 1),
         nlf(mu2 ~ a2 + b2 * 2),
         a1 + a2 + b1 + b2 ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b1),
                prior(normal(0, 0.5), class = b, nlpar = b2)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      file = "fits/m11.13a")

m11.13b <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b1 * 1),
         nlf(mu2 ~ a2 + b2 * 2),
         a1 + a2 + b1 + b2 ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b1, lb = 0),
                prior(normal(0, 0.5), class = b, nlpar = b2, lb = 0)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      control = list(adapt_delta = .99),
      file = "fits/m11.13b")

m11.13c <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b * 1),
         nlf(mu2 ~ a2 + b * 2),
         a1 + a2 + b ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      file = "fits/m11.13c")

m11.13d <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b * 1),
         nlf(mu2 ~ a2 + b * 2),
         a1 + a2 + b ~ 1),
      prior = c(prior(normal(0, 1), class = b, nlpar = a1),
                prior(normal(0, 1), class = b, nlpar = a2),
                prior(normal(0, 0.5), class = b, nlpar = b, lb = 0)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      control = list(adapt_delta = .99),
      file = "fits/m11.13d")
```

```{r, eval = F, echo = F}
print(m11.13a)
print(m11.13b)
print(m11.13c)
print(m11.13d)
```


I'm not going to exhaustively show the `print()` output for each. If you check, you'll see they all fit reasonably well. Here we'll look at their parameter summaries in bulk with a coefficient plot.


```{r, fig.width = 7, fig.height = 1.25}
tibble(fit = str_c("m11.13", letters[1:4])) |> 
  mutate(fixef = purrr::map(fit, ~ get(.) |> 
                              fixef() |>
                              data.frame() |> 
                              rownames_to_column("parameter"))) |> 
  unnest(fixef) |> 
  mutate(parameter = str_remove(parameter, "_Intercept"),
         fit       = factor(fit, levels = str_c("m11.13", letters[4:1]))) |> 
  
  ggplot(aes(x = Estimate, xmin = Q2.5, xmax = Q97.5, y = fit)) +
  geom_vline(xintercept = 0, color = wes_palette("Moonrise2")[3]) +
  geom_pointrange(fatten = 3/2, color = wes_palette("Moonrise2")[4]) +
  ylab(NULL) +
  theme(axis.ticks.y = element_blank(),
        panel.background = element_rect(fill = alpha("white", 1/8), linewidth = 0)) +
  facet_wrap(~ parameter, nrow = 1)
```


The results differed across models. None of them match up with the results McElreath reported in the text. However, the parameters from `m11.13d` are very close to those from our `m11.13`.


```{r}
precis(m11.13, depth = 2)
fixef(m11.13d) |> round(digits = 2)
```


It might be instructive to compare `m11.13a` through `m11.13d` with the PSIS-LOO.


```{r}
#| message: false
#| warning: false

m11.13a <- add_criterion(m11.13a, "loo")
m11.13b <- add_criterion(m11.13b, "loo")
m11.13c <- add_criterion(m11.13c, "loo")
m11.13d <- add_criterion(m11.13d, "loo")

loo_compare(m11.13a, m11.13b, m11.13c, m11.13d, criterion = "loo") |> 
  print(simplify = F)
model_weights(m11.13a, m11.13b, m11.13c, m11.13d, weights = "loo") |> 
  round(digits = 2)
```


Two things pop out, here. First, all models are essentially equivalent in terms of LOO estimates and LOO weights. Second, the effective number of parameters ($p_\text{LOO}$) is about 2 for each model. At first glance, this might be surprising given that `m11.13a` and `m11.13b` both have 4 parameters and `m11.13c` and `m11.13d` both have three parameters. But recall that none of these models contain predictor variables from the data. All those $\beta$ parameters, whether they're held equal or allowed to vary across $s_1$ and $s_2$, are just constants. In the absence of actual `income` values that vary within the data, those $\beta$ parameters are kinda like extra intercepts. For context, go back and review our multicollinear legs from [Section 6.1.1][Multicollinear legs.] or our double intercepts from [Section 9.5.4][Non-identifiable parameters.].

Now see what happens when we compare these four models with our intercepts-only model, `m11.13io`.


```{r, warning = F, message = F}
m11.13io <- add_criterion(m11.13io, "loo")

loo_compare(m11.13io, m11.13a, m11.13b, m11.13c, m11.13d, criterion = "loo") |> 
  print(simplify = F)
model_weights(m11.13io, m11.13a, m11.13b, m11.13c, m11.13d, weights = "loo") |> 
  round(digits = 2)
```


They're all the same. Each model effectively has 2 parameters. Though it doesn't do much by way of cross-validation, McElreath's extra $\beta$ parameter will let us perform a counterfactual simulation. Here is a **brms**/**tidyverse** workflow to make a counterfactual simulation for two levels of `income` based on our `m11.13d`, the **brms** model most closely corresponding to our **rethinking**-based `m11.13`.


```{r, warning = F}
as_draws_df(m11.13d) |> 
  transmute(s1      = b_a1_Intercept + b_b_Intercept * income[1],
            s2_orig = b_a2_Intercept + b_b_Intercept * income[2],
            s2_new  = b_a2_Intercept + b_b_Intercept * income[2] * 2) |> 
  mutate(p_orig = purrr::map2_dbl(s1, s2_orig, ~softmax(.x, .y, 0)[2]),
         p_new  = purrr::map2_dbl(s1, s2_new, ~softmax(.x, .y, 0)[2])) |> 
  mutate(p_diff = p_new - p_orig) |> 
  mean_qi(p_diff) |> 
  mutate_if(is.double, round, digits = 2)
```


Now let's build.

### Predictors matched to observations.


```{r, warning = F, message = F}
n <- 500
set.seed(11)

# simulate family incomes for each individual
family_income <- runif(n)

# assign a unique coefficient for each type of event
b      <- c(-2, 0, 2)
career <- rep(NA, n)  # empty vector of choices for each individual
for (i in 1:n) {
    score     <- 0.5 * (1:3) + b * family_income[i]
    p         <- softmax(score[1], score[2], score[3])
    career[i] <- sample(1:3, size = 1, prob = p)
}
```


In effect, we now have three data-generating equations:

$$
\begin{align*}
s_1 & = 0.5 + -2 \cdot \text{family_income}_i \\
s_2 & = 1.0 +  0 \cdot \text{family_income}_i \\ 
s_3 & = 1.5 +  2 \cdot \text{family_income}_i,
\end{align*}
$$

where, because `family_income` is an actual variable that can take on unique values for each row in the data, we can call the first term in each equation the $\alpha$ parameter and the second term in each equation the $\beta$ parameter AND those $\beta$ parameters will be more than odd double intercepts.

We might examine what the `family_income` distributions look like across the three levels of `career`. We'll do it in two plots and combine them with the **patchwork** syntax. The first will be overlapping densities. For the second, we'll display the proportions of `career` across a discretized version of `family_income` in a stacked area plot.


```{r, fig.width = 7, fig.height = 3}
# put the data in a tibble
d <-
  tibble(career = career) |> 
  mutate(family_income = family_income)

p1 <-
  d |> 
  mutate(career = as.factor(career)) |> 
  
  ggplot(aes(x = family_income, fill = career)) +
  geom_density(linewidth = 0, alpha = 3/4) +
  scale_fill_manual(values = wes_palette("Moonrise2")[c(4, 2, 1)]) +
  theme(legend.position = "none")
  
p2 <-
  d |> 
  mutate(career = as.factor(career)) |>
  
  mutate(fi = santoku::chop_width(family_income, width = .1, start = 0, labels = 1:10)) |> 
  count(fi, career) |> 
  group_by(fi) |> 
  mutate(proportion = n / sum(n)) |> 
  mutate(f = as.double(fi)) |> 
  
  ggplot(aes(x = (f - 1) / 9, y = proportion, fill = career)) +
  geom_area() +
  scale_fill_manual(values = wes_palette("Moonrise2")[c(4, 2, 1)]) +
  xlab("family_income, descritized")

p1 + p2
```


Since Mcelreath's simulation code in McElreath's **R** code 11.59 did not contain a `set.seed()` line, it won't be possible to exactly reproduce his results. Happily, though, it appears that this time the results he reported in the text to cohere reasonably well with I ran the code on my computer. They weren't identical, but there were much closer that for `m11.13` from the last section. Since things are working more smoothly, here, I'm going to jump directly to **brms** code.


```{r m11.14}
m11.14 <-
  brm(data = d, 
      family = categorical(link = logit, refcat = 3),
      bf(career ~ 1,
         nlf(mu1 ~ a1 + b1 * family_income),
         nlf(mu2 ~ a2 + b2 * family_income),
         a1 + a2 + b1 + b2 ~ 1),
      prior = c(prior(normal(0, 1.5), class = b, nlpar = a1),
                prior(normal(0, 1.5), class = b, nlpar = a2),
                prior(normal(0, 1), class = b, nlpar = b1),
                prior(normal(0, 1), class = b, nlpar = b2)),
      iter = 2000, warmup = 1000, cores = 4, chains = 4,
      seed = 11,
      file = "fits/m11.14")
```

```{r}
print(m11.14)
```


Check the PSIS-LOO.


```{r, warning = F, message = F}
m11.14 <- add_criterion(m11.14, "loo")

loo(m11.14)
```


Now that we actually have predictor variables with which we might estimate conventional $\beta$ parameters, we finally have more than 2 effective parameters ($p_\text{LOO}$).

"Again, computing implied predictions is the safest way to interpret these models. They do a great job of classifying discrete, unordered events. But the parameters are on a scale that is very hard to interpret" (p. 325). Like before, we'll do that with `fitted()`. Now we have a predictor, this time we will use the `newdata` argument.


```{r}
nd <- tibble(family_income = seq(from = 0, to = 1, length.out = 60))

f <-
  fitted(m11.14,
         newdata = nd)
```


First we'll plot the fitted probabilities for each `career` level across the full range of `family_income` values.


```{r, fig.width = 7, fig.height = 2.75}
# wrangle
rbind(f[, , 1],
      f[, , 2],
      f[, , 3]) |> 
  data.frame() |> 
  bind_cols(expand_grid(career = 1:3, nd)) |> 
  mutate(career = str_c("career: ", career)) |> 
  
  # plot
  ggplot(aes(x = family_income, y = Estimate,
             ymin = Q2.5, ymax = Q97.5,
             fill = career, color = career)) +
  geom_ribbon(alpha = 2/3, linewidth = 0) +
  geom_line(linewidth = 3/4) +
  scale_fill_manual(values = wes_palette("Moonrise2")[c(4, 2, 1)]) +
  scale_color_manual(values = wes_palette("Moonrise2")[c(4, 2, 1)]) +
  scale_x_continuous(breaks = 0:2 / 2) +
  scale_y_continuous("probability", limits = c(0, 1),
                     breaks = 0:3 / 3, labels = c("0", ".33", ".67", "1")) +
  theme(axis.text.y = element_text(hjust = 0),
        legend.position = "none") +
  facet_wrap(~ career)
```


If we're willing to summarize those fitted lines by their posterior means, we could also make a model-implied version of the stacked area plot from above.


```{r, fig.width = 3.25, fig.height = 3}
# annotation
text <-
  tibble(family_income = c(.45, .3, .15),
         proportion    = c(.65, .8, .95),
         label         = str_c("career: ", 3:1),
         color         = c("a", "a", "b"))

# wrangle
rbind(f[, , 1],
      f[, , 2],
      f[, , 3]) |> 
  data.frame() |> 
  bind_cols(expand_grid(career = 1:3, nd)) |> 
  group_by(family_income) |> 
  mutate(proportion = Estimate / sum(Estimate),
         career     = factor(career)) |> 
  
  # plot!
  ggplot(aes(x = family_income, y = proportion)) +
  geom_area(aes(fill = career)) +
  geom_text(data = text,
            aes(label = label, color = color),
            family = "Times", size = 4.25) +
  scale_color_manual(values = wes_palette("Moonrise2")[4:3]) +
  scale_fill_manual(values = wes_palette("Moonrise2")[c(4, 2, 1)]) +
  theme(legend.position = "none")
```


For more practice fitting multinomial models with **brms**, check out [Chapter 22](https://bookdown.org/content/3686/nominal-predicted-variable.html) of my [-@kurzDoingBayesianDataAnalysis2023] translation of Kruschke's [-@kruschkeDoingBayesianData2015] text.

#### Multinomial in disguise as Poisson.

Here we fit a multinomial likelihood by refactoring it to a series of Poisson models. Let's retrieve the Berkeley data.


```{r}
data(UCBadmit, package = "rethinking")
d <- UCBadmit
rm(UCBadmit)
```


Set up the `stan_data`.


```{r}
stan_data <- d |> 
  # `reject` is a reserved word in Stan, cannot use as variable name
  mutate(rej = reject) |> 
  select(admit, rej, applications) |> 
  compose_data()

# What?
str(stan_data)
```


Define the `model_code` objects for the two complimentary models.


```{r}
# Binomial model of overall admission probability
model_code_11.binom <- '
data {
  int<lower=1> n;
  array[n] int<lower=1> applications;
  array[n] int<lower=0> admit;
}
parameters {
  real a;
}
model {
  admit ~ binomial(applications, inv_logit(a));
  a ~ normal(0, 1.5);
}
'

# Poisson model of overall admission rate and rejection rate
model_code_11.pois <- '
data {
  int<lower=1> n;
  array[n] int<lower=1> applications;
  array[n] int<lower=0> admit;
  array[n] int<lower=0> rej;
}
parameters {
  real a1;
  real a2;
}
model {
  admit ~ poisson(exp(a1));
  rej ~ poisson(exp(a2));
  [a1, a2] ~ normal(0, 1.5);
}
'
```


Compile and extract the posterior draws for both, with `stan()`.


```{r}
m11.binom <- stan(
  model_code = model_code_11.binom, 
  data = stan_data,
  chains = 3, cores = 3, seed = 11)

m11.pois <- stan(
  model_code = model_code_11.pois,
  data = stan_data,
  chains = 3, cores = 3, seed = 11)
```


Check the model summaries.


```{r}
print(m11.binom, pars = "a", probs = c(0.055, 0.945))
print(m11.pois, pars = c("a1", "a2"), probs = c(0.055, 0.945))
```


Though the model summaries look very different for the two models, they give the same answer, within MCMC simulation error, for the focal estimand $p_\text{admit}$. We might explore that in a plot.


```{r, fig.width = 4, fig.height = 2.5, warning = F}
bind_rows(
  # Binomial
  as_draws_df(m11.binom) |> 
    transmute(p_admit = plogis(a)),
  # Poisson
  as_draws_df(m11.pois) |> 
    transmute(p_admit = exp(a1) / (exp(a1) + exp(a2)))
) |> 
  mutate(fit = rep(c("m11.binom", "m11.pois"), each = n() / 2)) |> 
  
  ggplot(aes(x = p_admit, y = fit)) +
  stat_halfeye(.width = 0.89) +
  scale_y_discrete(NULL, expand = expansion(mult = 0.1)) +
  xlab(expression(italic(p)[admit]))
```


Though we have different model types, they gave the same results for the focal estimand.

#### Overthinking: Multinomial-Poisson transformation.

## Summary

## Session info {-}




